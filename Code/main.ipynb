{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# List of the 10 specific classes to detect\n",
    "allowed_classes = ['cell phone', 'remote', 'knife', 'book', 'spoon', 'cup', 'scissors', 'fork', 'toothbrush', 'ball']\n",
    "model = YOLO('yolo11x.pt') \n",
    "\n",
    "# Predefined unique colors for the 10 classes\n",
    "box_colors = {\n",
    "    'cell phone': (255, 0, 0),\n",
    "    'remote': (0, 255, 0),\n",
    "    'knife': (0, 0, 255),\n",
    "    'book': (255, 255, 0),\n",
    "    'spoon': (255, 0, 255),\n",
    "    'cup': (0, 255, 255),\n",
    "    'scissors': (128, 0, 128),\n",
    "    'fork': (0, 128, 128),\n",
    "    'toothbrush': (128, 128, 0),\n",
    "    'ball': (0, 128, 0)\n",
    "}\n",
    "\n",
    "ground_truth = {\n",
    "    'S1_front': ['O1'],\n",
    "    'S1_left': ['O1'],\n",
    "    'S1_right': ['O1'],\n",
    "    'S2_front': ['O1', 'O5'],\n",
    "    'S2_left': ['O1', 'O5'],\n",
    "    'S2_right': ['O1', 'O5'],\n",
    "    'S3_front': ['O1', 'O2', 'O5'],\n",
    "    'S3_left': ['O1', 'O2', 'O5'],\n",
    "    'S3_right': ['O1', 'O2', 'O5'],\n",
    "    'S4_front': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S4_left': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S4_right': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S5_front': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S5_left': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S5_right': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S6_front': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S6_left': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S6_right': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S7_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S7_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S7_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S8_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S8_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S8_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S9_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S9_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S9_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S10_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "    'S10_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "    'S10_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "}\n",
    "\n",
    "resulting_accuracy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_object_name(name):\n",
    "    mappings = {\n",
    "        'cell phone': 'O1',\n",
    "        'remote': 'O2',\n",
    "        'knife': 'O3',\n",
    "        'book': 'O4',\n",
    "        'spoon': 'O5',\n",
    "        'cup': 'O6',\n",
    "        'scissors': 'O7',\n",
    "        'fork': 'O8',\n",
    "        'toothbrush': 'O9',\n",
    "        'ball': 'O10'\n",
    "    }\n",
    "\n",
    "    return mappings[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(results, scene_name):\n",
    "    # Iterate through detected objects and draw bounding boxes for allowed classes\n",
    "    for box in results[0].boxes:\n",
    "        label = int(box.cls.item())\n",
    "        name = model.names[label]\n",
    "\n",
    "        drawn_classes = set()\n",
    "        \n",
    "        # Only process if the detected class is in the allowed list and not already drawn\n",
    "        if name in allowed_classes and name not in drawn_classes:\n",
    "            object_number = map_object_name(name)\n",
    "            drawn_classes.add(name)\n",
    "\n",
    "            if scene_name not in resulting_accuracy:\n",
    "                resulting_accuracy[scene_name] = [object_number]\n",
    "            else:\n",
    "                resulting_accuracy[scene_name].append(object_number)\n",
    "    \n",
    "    with open('../testing.txt', 'a') as f:\n",
    "        f.write(f'Scene: {scene_name}\\n')\n",
    "        f.write('Detected Objects: {}\\n\\n'.format(', '.join(\n",
    "            sorted(resulting_accuracy[scene_name])\n",
    "        )))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    results_per_scene = []\n",
    "    for scene_name in ground_truth:\n",
    "        detected_objects = resulting_accuracy[scene_name]\n",
    "        actual_objects = ground_truth[scene_name]\n",
    "        TP = len(set(actual_objects).intersection(set(detected_objects)))\n",
    "        FP = len(set(detected_objects) - set(actual_objects))\n",
    "        FN = len(set(actual_objects) - set(detected_objects))\n",
    "        TN = len(allowed_classes) - TP - FP - FN\n",
    "\n",
    "        results_per_scene.append({\n",
    "            'Scene': scene_name,\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'TN': TN,\n",
    "            'FN': FN\n",
    "        })\n",
    "    \n",
    "    # Calculate overall precision, recall, F1-score, and accuracy\n",
    "    total_TP = sum([res['TP'] for res in results_per_scene])\n",
    "    total_FP = sum([res['FP'] for res in results_per_scene])\n",
    "    total_TN = sum([res['TN'] for res in results_per_scene])\n",
    "    total_FN = sum([res['FN'] for res in results_per_scene])\n",
    "\n",
    "    precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "    recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (total_TP + total_TN) / (total_TP + total_FP + total_TN + total_FN) if (\n",
    "                total_TP + total_FP + total_TN + total_FN) > 0 else 0\n",
    "\n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "    print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "    print(\"F1-Score: {:.2f}%\".format(f1_score * 100))\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(results, image_path):\n",
    "    # Get the original image for plotting\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Set to keep track of already drawn classes\n",
    "    drawn_classes = set()\n",
    "\n",
    "    # Iterate through detected objects and draw bounding boxes for allowed classes\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        label = int(box.cls.item())\n",
    "        name = model.names[label]\n",
    "\n",
    "        # Only process if the detected class is in the allowed list and not already drawn\n",
    "        if name in allowed_classes and name not in drawn_classes:\n",
    "            color = box_colors[name]  # Get the predefined color for this class\n",
    "\n",
    "            # Draw the bounding box on the image with thicker lines\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 12)  # Increased thickness\n",
    "\n",
    "            # Display the name above the bounding box with larger text\n",
    "            cv2.putText(image, name, (x1, y1 - 15), cv2.FONT_HERSHEY_SIMPLEX, 2.5, color, 3)  # Larger font size and thickness\n",
    "\n",
    "            # Add the class to the drawn set\n",
    "            drawn_classes.add(name)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S1_front.jpg: 640x480 1 cell phone, 306.5ms\n",
      "Speed: 6.7ms preprocess, 306.5ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S1_left.jpg: 640x480 1 cell phone, 258.1ms\n",
      "Speed: 1.4ms preprocess, 258.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S1_right.jpg: 640x480 1 cell phone, 255.1ms\n",
      "Speed: 1.4ms preprocess, 255.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S2_front.jpg: 480x640 1 fork, 1 spoon, 1 cell phone, 258.4ms\n",
      "Speed: 1.4ms preprocess, 258.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S2_left.jpg: 480x640 1 spoon, 1 cell phone, 255.0ms\n",
      "Speed: 1.3ms preprocess, 255.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S2_right.jpg: 576x640 1 spoon, 1 dining table, 1 cell phone, 313.5ms\n",
      "Speed: 1.6ms preprocess, 313.5ms inference, 0.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S3_front.jpg: 640x480 1 spoon, 1 remote, 1 cell phone, 270.1ms\n",
      "Speed: 1.4ms preprocess, 270.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S3_left.jpg: 640x480 1 spoon, 1 remote, 1 cell phone, 322.3ms\n",
      "Speed: 1.5ms preprocess, 322.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S3_right.jpg: 640x544 1 spoon, 2 cell phones, 303.7ms\n",
      "Speed: 1.7ms preprocess, 303.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S4_front.jpg: 640x640 1 knife, 1 spoon, 1 remote, 1 cell phone, 389.0ms\n",
      "Speed: 1.7ms preprocess, 389.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S4_left.jpg: 640x480 1 knife, 1 remote, 1 cell phone, 278.9ms\n",
      "Speed: 1.4ms preprocess, 278.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S4_right.jpg: 640x640 1 knife, 1 spoon, 1 dining table, 1 remote, 2 cell phones, 354.4ms\n",
      "Speed: 1.7ms preprocess, 354.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S5_front.jpg: 640x480 1 cup, 1 knife, 1 spoon, 1 remote, 1 cell phone, 268.8ms\n",
      "Speed: 1.5ms preprocess, 268.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S5_left.jpg: 640x480 1 cup, 1 knife, 1 remote, 1 cell phone, 279.7ms\n",
      "Speed: 1.5ms preprocess, 279.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S5_right.jpg: 544x640 1 cup, 1 knife, 1 spoon, 1 dining table, 2 cell phones, 291.7ms\n",
      "Speed: 1.5ms preprocess, 291.7ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S6_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 259.8ms\n",
      "Speed: 1.5ms preprocess, 259.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S6_left.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 278.8ms\n",
      "Speed: 1.6ms preprocess, 278.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S6_right.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 281.8ms\n",
      "Speed: 1.6ms preprocess, 281.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S7_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 255.0ms\n",
      "Speed: 1.4ms preprocess, 255.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S7_left.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 remote, 1 cell phone, 1 book, 256.0ms\n",
      "Speed: 1.4ms preprocess, 256.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S7_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 2 cell phones, 1 book, 281.9ms\n",
      "Speed: 1.5ms preprocess, 281.9ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S8_front.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 284.5ms\n",
      "Speed: 1.5ms preprocess, 284.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S8_left.jpg: 640x512 1 knife, 1 cell phone, 1 scissors, 281.7ms\n",
      "Speed: 1.5ms preprocess, 281.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S8_right.jpg: 640x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 1 scissors, 342.3ms\n",
      "Speed: 1.7ms preprocess, 342.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S9_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 258.2ms\n",
      "Speed: 1.5ms preprocess, 258.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S9_left.jpg: 608x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 313.6ms\n",
      "Speed: 1.7ms preprocess, 313.6ms inference, 0.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S9_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 1 book, 1 scissors, 278.7ms\n",
      "Speed: 1.6ms preprocess, 278.7ms inference, 0.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S10_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 1 toothbrush, 255.6ms\n",
      "Speed: 1.5ms preprocess, 255.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S10_left.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 cell phone, 1 scissors, 1 toothbrush, 262.7ms\n",
      "Speed: 1.4ms preprocess, 262.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Scenes/S10_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 1 toothbrush, 280.8ms\n",
      "Speed: 1.6ms preprocess, 280.8ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    }
   ],
   "source": [
    "# Clear the file before starting\n",
    "with open('../testing.txt', 'w') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "for i in range(len(allowed_classes)):\n",
    "    for side in [\"front\", \"left\", \"right\"]:\n",
    "        scene_name = f\"S{i + 1}_{side}\"\n",
    "        image_path = f\"../Scenes/{scene_name}.jpg\"\n",
    "\n",
    "        # Perform object detection\n",
    "        results = model(image_path)\n",
    "\n",
    "        # Extract the image with bounding boxes drawn\n",
    "        # detected_image = results[0].plot()  # Get the image with bounding boxes drawn\n",
    "        calculate_results(results, scene_name)\n",
    "        image = draw_bounding_box(results, image_path)\n",
    "        # Display the detected image\n",
    "        # display_image(detected_image, \"Detected Objects in Scene\")\n",
    "\n",
    "        output_path = f\"../Detected Objects/{scene_name}_bb.jpg\"\n",
    "        cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Results:\n",
      "Precision: 99.30%\n",
      "Recall: 86.06%\n",
      "F1-Score: 92.21%\n",
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class ImageStitching:\n",
    "    \"\"\"containts the utilities required to stitch images\"\"\"\n",
    "\n",
    "    def __init__(self, query_photo, train_photo):\n",
    "        super().__init__()\n",
    "        width_query_photo = query_photo.shape[1]\n",
    "        width_train_photo = train_photo.shape[1]\n",
    "        lowest_width = min(width_query_photo, width_train_photo)\n",
    "        smoothing_window_percent = 0.10 # consider increasing or decreasing[0.00, 1.00] \n",
    "        self.smoothing_window_size = max(100, min(smoothing_window_percent * lowest_width, 1000))\n",
    "\n",
    "    def convert_grayscale(self, image):\n",
    "        photo_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        return image, photo_gray\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_detector(image):\n",
    "        \"\"\"Applies SIFT algorithm to the given image\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): input image\n",
    "\n",
    "        Returns:\n",
    "            keypoints, features\n",
    "        \"\"\"\n",
    "        descriptor = cv2.SIFT_create()\n",
    "        keypoints, features = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        return keypoints, features\n",
    "\n",
    "    def create_and_match_keypoints(self, features_train_image, features_query_image):\n",
    "        \"\"\"Creates and Matches keypoints from the SIFT features using Brute Force matching\n",
    "        by checking the L2 norm of the feature vector\n",
    "\n",
    "        Args:\n",
    "            features_train_image: SIFT features of train image\n",
    "            features_query_image: SIFT features of query image\n",
    "\n",
    "        Returns:\n",
    "            matches (List): matches in features of train and query image\n",
    "        \"\"\"\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "        best_matches = bf.match(features_train_image, features_query_image)\n",
    "        raw_matches = sorted(best_matches, key=lambda x: x.distance)\n",
    "\n",
    "        return raw_matches\n",
    "\n",
    "    def compute_homography(\n",
    "        self, keypoints_train_image, keypoints_query_image, matches, reprojThresh\n",
    "    ):\n",
    "        \"\"\"Computes the Homography to map images to a single plane,\n",
    "        uses RANSAC algorithm to find the best matches iteratively.\n",
    "\n",
    "        Args:\n",
    "            keypoints_train_image: keypoints found using SIFT in train image\n",
    "            keypoints_query_image: keypoints found using SIFT in query image\n",
    "            matches: matches found using Brute Force\n",
    "            reprojThresh: threshold for error\n",
    "\n",
    "        Returns:\n",
    "            M (Tuple): (matches, Homography matrix, status)\n",
    "        \"\"\"\n",
    "        keypoints_train_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_train_image]\n",
    "        )\n",
    "        keypoints_query_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_query_image]\n",
    "        )\n",
    "\n",
    "        if len(matches) >= 4:\n",
    "            points_train = np.float32(\n",
    "                [keypoints_train_image[m.queryIdx] for m in matches]\n",
    "            )\n",
    "            points_query = np.float32(\n",
    "                [keypoints_query_image[m.trainIdx] for m in matches]\n",
    "            )\n",
    "\n",
    "            H, status = cv2.findHomography(\n",
    "                points_train, points_query, cv2.RANSAC, reprojThresh\n",
    "            )\n",
    "\n",
    "            return (matches, H, status)\n",
    "\n",
    "        else:\n",
    "            print(f\"Minimum match count not satisfied cannot get homopgrahy\")\n",
    "            return None\n",
    "\n",
    "    def create_mask(self, query_image, train_image, version):\n",
    "        \"\"\"Creates the mask using query and train images for blending the images,\n",
    "        using a gaussian smoothing window/kernel\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            version (str) == 'left_image' or 'right_image'\n",
    "\n",
    "        Returns:\n",
    "            masks\n",
    "        \"\"\"\n",
    "        height_query_photo = query_image.shape[0]\n",
    "        width_query_photo = query_image.shape[1]\n",
    "        width_train_photo = train_image.shape[1]\n",
    "        height_panorama = height_query_photo\n",
    "        width_panorama = width_query_photo + width_train_photo\n",
    "        offset = int(self.smoothing_window_size / 2)\n",
    "        barrier = query_image.shape[1] - int(self.smoothing_window_size / 2)\n",
    "        mask = np.zeros((height_panorama, width_panorama))\n",
    "        if version == \"left_image\":\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(1, 0, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, : barrier - offset] = 1\n",
    "        else:\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(0, 1, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, barrier + offset :] = 1\n",
    "        return cv2.merge([mask, mask, mask])\n",
    "\n",
    "    def blending_smoothing(self, query_image, train_image, homography_matrix):\n",
    "        \"\"\"blends both query and train image via the homography matrix,\n",
    "        and ensures proper blending and smoothing using masks created in create_masks()\n",
    "        to give a seamless panorama.\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            homography_matrix (numpy array): Homography to map images to a single plane\n",
    "\n",
    "        Returns:\n",
    "            panoramic image (numpy array)\n",
    "        \"\"\"\n",
    "        height_img1 = query_image.shape[0]\n",
    "        width_img1 = query_image.shape[1]\n",
    "        width_img2 = train_image.shape[1]\n",
    "        height_panorama = height_img1\n",
    "        width_panorama = width_img1 + width_img2\n",
    "\n",
    "        panorama1 = np.zeros((height_panorama, width_panorama, 3))\n",
    "        mask1 = self.create_mask(query_image, train_image, version=\"left_image\")\n",
    "        panorama1[0 : query_image.shape[0], 0 : query_image.shape[1], :] = query_image\n",
    "        panorama1 *= mask1\n",
    "        mask2 = self.create_mask(query_image, train_image, version=\"right_image\")\n",
    "        panorama2 = (\n",
    "            cv2.warpPerspective(\n",
    "                train_image, homography_matrix, (width_panorama, height_panorama)\n",
    "            )\n",
    "            * mask2\n",
    "        )\n",
    "        result = panorama1 + panorama2\n",
    "\n",
    "        # remove extra blackspace\n",
    "        rows, cols = np.where(result[:, :, 0] != 0)\n",
    "        min_row, max_row = min(rows), max(rows) + 1\n",
    "        min_col, max_col = min(cols), max(cols) + 1\n",
    "\n",
    "        final_result = result[min_row:max_row, min_col:max_col, :]\n",
    "\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def forward(query_photo, train_photo):\n",
    "    \"\"\"Runs a forward pass using the ImageStitching() class in utils.py.\n",
    "    Takes in a query image and train image and runs entire pipeline to return\n",
    "    a panoramic image.\n",
    "\n",
    "    Args:\n",
    "        query_photo (numpy array): query image\n",
    "        train_photo (nnumpy array): train image\n",
    "\n",
    "    Returns:\n",
    "        result image (numpy array): RGB result image\n",
    "    \"\"\"\n",
    "    image_stitching = ImageStitching(query_photo, train_photo)\n",
    "    _, query_photo_gray = image_stitching.convert_grayscale(query_photo)  # left image\n",
    "    _, train_photo_gray = image_stitching.convert_grayscale(train_photo)  # right image\n",
    "\n",
    "    keypoints_train_image, features_train_image = image_stitching._sift_detector(\n",
    "        train_photo_gray\n",
    "    )\n",
    "    keypoints_query_image, features_query_image = image_stitching._sift_detector(\n",
    "        query_photo_gray\n",
    "    )\n",
    "\n",
    "    matches = image_stitching.create_and_match_keypoints(\n",
    "        features_train_image, features_query_image\n",
    "    )\n",
    "\n",
    "    mapped_feature_image = cv2.drawMatches(\n",
    "                        train_photo,\n",
    "                        keypoints_train_image,\n",
    "                        query_photo,\n",
    "                        keypoints_query_image,\n",
    "                        matches[:100],\n",
    "                        None,\n",
    "                        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    M = image_stitching.compute_homography(\n",
    "        keypoints_train_image, keypoints_query_image, matches, reprojThresh=4\n",
    "    )\n",
    "\n",
    "    if M is None:\n",
    "        return \"Error cannot stitch images\"\n",
    "\n",
    "    (matches, homography_matrix, status) = M\n",
    "\n",
    "    result = image_stitching.blending_smoothing(\n",
    "        query_photo, train_photo, homography_matrix\n",
    "    )\n",
    "    # mapped_image = cv2.drawMatches(train_photo, keypoints_train_image, query_photo, keypoints_query_image, matches[:100], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    mapped_float_32 = np.float32(mapped_feature_image)\n",
    "    result_float32 = np.float32(result)\n",
    "    result_rgb = cv2.cvtColor(result_float32, cv2.COLOR_BGR2RGB)\n",
    "    mapped_feature_image_rgb = cv2.cvtColor(mapped_float_32, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return result_rgb, mapped_feature_image_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def stich_images(image_list, no_of_images):\n",
    "    result, mapped_image = forward(\n",
    "        query_photo=image_list[no_of_images - 2],\n",
    "        train_photo=image_list[no_of_images - 1],\n",
    "    )\n",
    "\n",
    "    mapped_image_int8 = np.uint8(mapped_image)\n",
    "    mapped_image_rgb = cv2.cvtColor(mapped_image_int8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result_int8 = np.uint8(result)\n",
    "    result_rgb = cv2.cvtColor(result_int8, cv2.COLOR_BGR2RGB)\n",
    "    return result_rgb, mapped_image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def panorama_main(image_list):\n",
    "    \"\"\"Main function of the Repository.\n",
    "    Automatically uses the images in the specified directory \n",
    "    to create and export a panoramic image in the /outputs/ folder.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Directory containing input images\n",
    "    \"\"\"\n",
    "    result = image_list[0]\n",
    "    temp_list = []\n",
    "    for i in range(1, len(image_list)):\n",
    "        temp_list = [result, image_list[i]]\n",
    "        # Process images to create a panorama\n",
    "        result, mapped_image = stich_images(temp_list, len(temp_list))\n",
    "        temp_list = []\n",
    "    \n",
    "    # Save the results to the outputs folder\n",
    "    os.makedirs(\"../Panorama\", exist_ok=True)\n",
    "    cv2.imwrite(\"../Panorama/Panorama.jpg\", result)\n",
    "    # cv2.imwrite(\"outputs/mapped_image.jpg\", mapped_image)\n",
    "\n",
    "    print(f\"Panoramic image saved at: outputs/panorama_image.jpg\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoramic image saved at: outputs/panorama_image.jpg\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../Temp3\"\n",
    "image_list = []\n",
    "for i in range(10):\n",
    "    scene_name = f\"S{i + 1}\"\n",
    "    image_path = f\"{image_dir}/{scene_name}.jpg\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        image_list.append(img)\n",
    "\n",
    "panorama_main(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Project/Code/../Panorama/Panorama.jpg: 352x640 1 cup, 2 forks, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 1 toothbrush, 738.8ms\n",
      "Speed: 12.8ms preprocess, 738.8ms inference, 11.7ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = f\"../Panorama/Panorama.jpg\"\n",
    "\n",
    "# Perform object detection\n",
    "results = model(image_path)\n",
    "\n",
    "# Extract the image with bounding boxes drawn\n",
    "# detected_image = results[0].plot()  # Get the image with bounding boxes drawn\n",
    "image = draw_bounding_box(results, image_path)\n",
    "# Display the detected image\n",
    "# display_image(detected_image, \"Detected Objects in Scene\")\n",
    "\n",
    "output_path = f\"../Panorama/Panorama_bb.jpg\"\n",
    "cv2.imwrite(output_path, image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
