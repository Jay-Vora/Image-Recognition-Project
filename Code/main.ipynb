{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# List of the 10 specific classes to detect\n",
    "allowed_classes = [\n",
    "    'cell phone', 'remote', 'knife', 'book', 'spoon',\n",
    "    'cup', 'scissors', 'fork', 'toothbrush', 'ball'\n",
    "]\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = YOLO('yolo11x.pt')\n",
    "\n",
    "# Predefined unique colors for bounding boxes of the 10 classes\n",
    "box_colors = {\n",
    "    'cell phone': (255, 0, 0),      # Blue\n",
    "    'remote': (0, 255, 0),          # Green\n",
    "    'knife': (0, 0, 255),           # Red\n",
    "    'book': (255, 255, 0),          # Cyan\n",
    "    'spoon': (255, 0, 255),         # Magenta\n",
    "    'cup': (0, 255, 255),           # Yellow\n",
    "    'scissors': (128, 0, 128),      # Purple\n",
    "    'fork': (0, 128, 128),          # Teal\n",
    "    'toothbrush': (128, 128, 0),    # Olive\n",
    "    'ball': (0, 128, 0)             # Dark Green\n",
    "}\n",
    "\n",
    "# Ground truth for object presence in different scenarios\n",
    "# Each key represents a scene\n",
    "# The value is the list of objects expected to be present in that scenario\n",
    "ground_truth = {\n",
    "    'S1_front': ['O1'],\n",
    "    'S1_left': ['O1'],\n",
    "    'S1_right': ['O1'],\n",
    "    'S2_front': ['O1', 'O5'],\n",
    "    'S2_left': ['O1', 'O5'],\n",
    "    'S2_right': ['O1', 'O5'],\n",
    "    'S3_front': ['O1', 'O2', 'O5'],\n",
    "    'S3_left': ['O1', 'O2', 'O5'],\n",
    "    'S3_right': ['O1', 'O2', 'O5'],\n",
    "    'S4_front': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S4_left': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S4_right': ['O1', 'O2', 'O3', 'O5'],\n",
    "    'S5_front': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S5_left': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S5_right': ['O1', 'O2', 'O3', 'O5', 'O6'],\n",
    "    'S6_front': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S6_left': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S6_right': ['O1', 'O2', 'O3', 'O5', 'O6', 'O8'],\n",
    "    'S7_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S7_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S7_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O8'],\n",
    "    'S8_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S8_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S8_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8'],\n",
    "    'S9_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S9_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S9_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O10'],\n",
    "    'S10_front': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "    'S10_left': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "    'S10_right': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n",
    "}\n",
    "\n",
    "# Dictionary to store the resulting accuracy for each scenario\n",
    "resulting_accuracy = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_object_name(name):\n",
    "    \"\"\"\n",
    "    Maps an object name to its corresponding identifier.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the object (e.g., 'cell phone', 'remote').\n",
    "\n",
    "    Returns:\n",
    "        str: The identifier corresponding to the object name (e.g., 'O1', 'O2').\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the provided name does not exist in the mappings.\n",
    "    \"\"\"\n",
    "    mappings = {\n",
    "        'cell phone': 'O1',\n",
    "        'remote': 'O2',\n",
    "        'knife': 'O3',\n",
    "        'book': 'O4',\n",
    "        'spoon': 'O5',\n",
    "        'cup': 'O6',\n",
    "        'scissors': 'O7',\n",
    "        'fork': 'O8',\n",
    "        'toothbrush': 'O9',\n",
    "        'ball': 'O10'\n",
    "    }\n",
    "\n",
    "    return mappings[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(results, scene_name):\n",
    "    \"\"\"\n",
    "    Processes detection results for a given scene, calculates detected objects,\n",
    "    updates the accuracy log, and writes the comparison to a file.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of detection results from the YOLO model.\n",
    "                        Each detection result includes bounding boxes and class predictions.\n",
    "        scene_name (str): The name of the scene (e.g., 'S1_front') being evaluated.\n",
    "\n",
    "    Writes:\n",
    "        The comparison of detected objects and expected objects to a text file ('../testing.txt').\n",
    "    \"\"\"\n",
    "    # Keep track of detected classes for the current scene\n",
    "    drawn_classes = set()\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        label = int(box.cls.item())\n",
    "        name = model.names[label]\n",
    "\n",
    "        if name in allowed_classes and name not in drawn_classes:\n",
    "            object_number = map_object_name(name)\n",
    "            drawn_classes.add(name)\n",
    "\n",
    "            # Update the resulting accuracy dictionary\n",
    "            if scene_name not in resulting_accuracy:\n",
    "                resulting_accuracy[scene_name] = [object_number]\n",
    "            else:\n",
    "                resulting_accuracy[scene_name].append(object_number)\n",
    "\n",
    "    # Write results to testing.txt\n",
    "    with open('../testing.txt', 'a') as f:\n",
    "        f.write(f'Scene: {scene_name}\\n')\n",
    "        f.write('Detected Objects: {}\\n'.format(', '.join(\n",
    "            sorted(set(resulting_accuracy[scene_name]))\n",
    "        )))\n",
    "        f.write('Expected Objects: {}\\n\\n'.format(', '.join(\n",
    "            sorted(set(ground_truth[scene_name]))\n",
    "        )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    \"\"\"\n",
    "    Calculates the accuracy metrics for object detection across all scenes.\n",
    "    \n",
    "    The metrics include True Positives (TP), False Positives (FP), True Negatives (TN), \n",
    "    False Negatives (FN), precision, recall, F1-score, and overall accuracy.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the overall precision, recall, F1-score, and accuracy to the console.\n",
    "    \"\"\"\n",
    "    results_per_scene = []\n",
    "\n",
    "    for scene_name in ground_truth:\n",
    "        detected_objects = list(set(resulting_accuracy.get(scene_name, []))) # Empty if nothing is detected\n",
    "        actual_objects = ground_truth[scene_name]\n",
    "\n",
    "        # Calculate metrics for the scene\n",
    "        TP = len(set(actual_objects).intersection(set(detected_objects)))  # True Positives\n",
    "        FP = len(set(detected_objects) - set(actual_objects))              # False Positives\n",
    "        FN = len(set(actual_objects) - set(detected_objects))              # False Negatives\n",
    "        TN = len(allowed_classes) - TP - FP - FN                           # True Negatives\n",
    "\n",
    "        results_per_scene.append({\n",
    "            'Scene': scene_name,\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'TN': TN,\n",
    "            'FN': FN\n",
    "        })\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    total_TP = sum([res['TP'] for res in results_per_scene])\n",
    "    total_FP = sum([res['FP'] for res in results_per_scene])\n",
    "    total_TN = sum([res['TN'] for res in results_per_scene])\n",
    "    total_FN = sum([res['FN'] for res in results_per_scene])\n",
    "\n",
    "    precision = total_TP / (total_TP + total_FP)\n",
    "    recall = total_TP / (total_TP + total_FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (total_TP + total_TN) / (total_TP + total_FP + total_TN + total_FN)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "    print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "    print(\"F1-Score: {:.2f}%\".format(f1_score * 100))\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(results, image_path):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes around detected objects on the input image for allowed classes.\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of detection results from the YOLO model.\n",
    "                        Each result includes bounding boxes and class predictions.\n",
    "        image_path (str): Path to the input image on which bounding boxes will be drawn.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with bounding boxes and labels drawn for allowed classes.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to read the image from path: {image_path}\")\n",
    "\n",
    "    # Set to keep track of already drawn classes\n",
    "    drawn_classes = set()\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Convert bounding box coordinates to integers\n",
    "        label = int(box.cls.item())\n",
    "        name = model.names[label]\n",
    "\n",
    "        if name in allowed_classes and name not in drawn_classes:\n",
    "            color = box_colors[name]\n",
    "\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 12)\n",
    "            cv2.putText(\n",
    "                image, name, (x1, y1 - 15),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2.5, color, 3\n",
    "            )\n",
    "\n",
    "            drawn_classes.add(name)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S1_front.jpg: 640x480 1 cell phone, 373.1ms\n",
      "Speed: 10.3ms preprocess, 373.1ms inference, 13.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S1_left.jpg: 640x480 1 cell phone, 385.0ms\n",
      "Speed: 2.4ms preprocess, 385.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S1_right.jpg: 640x480 1 cell phone, 316.0ms\n",
      "Speed: 1.6ms preprocess, 316.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S2_front.jpg: 480x640 1 fork, 1 spoon, 1 cell phone, 290.5ms\n",
      "Speed: 1.5ms preprocess, 290.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S2_left.jpg: 480x640 1 spoon, 1 cell phone, 282.0ms\n",
      "Speed: 1.4ms preprocess, 282.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S2_right.jpg: 576x640 1 spoon, 1 dining table, 1 cell phone, 395.0ms\n",
      "Speed: 1.7ms preprocess, 395.0ms inference, 0.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S3_front.jpg: 640x480 1 spoon, 1 remote, 1 cell phone, 344.9ms\n",
      "Speed: 2.5ms preprocess, 344.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S3_left.jpg: 640x480 1 spoon, 1 remote, 1 cell phone, 338.1ms\n",
      "Speed: 1.6ms preprocess, 338.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S3_right.jpg: 640x544 1 spoon, 2 cell phones, 314.2ms\n",
      "Speed: 1.7ms preprocess, 314.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S4_front.jpg: 640x640 1 knife, 1 spoon, 1 remote, 1 cell phone, 397.8ms\n",
      "Speed: 1.9ms preprocess, 397.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S4_left.jpg: 640x480 1 knife, 1 remote, 1 cell phone, 293.8ms\n",
      "Speed: 2.0ms preprocess, 293.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S4_right.jpg: 640x640 1 knife, 1 spoon, 1 dining table, 1 remote, 2 cell phones, 397.4ms\n",
      "Speed: 1.8ms preprocess, 397.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S5_front.jpg: 640x480 1 cup, 1 knife, 1 spoon, 1 remote, 1 cell phone, 285.3ms\n",
      "Speed: 1.6ms preprocess, 285.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S5_left.jpg: 640x480 1 cup, 1 knife, 1 remote, 1 cell phone, 296.7ms\n",
      "Speed: 1.5ms preprocess, 296.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S5_right.jpg: 544x640 1 cup, 1 knife, 1 spoon, 1 dining table, 2 cell phones, 334.1ms\n",
      "Speed: 1.6ms preprocess, 334.1ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S6_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 267.5ms\n",
      "Speed: 2.0ms preprocess, 267.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S6_left.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 405.9ms\n",
      "Speed: 1.6ms preprocess, 405.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S6_right.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 326.1ms\n",
      "Speed: 1.8ms preprocess, 326.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S7_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 296.7ms\n",
      "Speed: 1.5ms preprocess, 296.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S7_left.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 remote, 1 cell phone, 1 book, 267.4ms\n",
      "Speed: 1.5ms preprocess, 267.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S7_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 2 cell phones, 1 book, 298.7ms\n",
      "Speed: 1.6ms preprocess, 298.7ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S8_front.jpg: 640x544 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 306.5ms\n",
      "Speed: 6.8ms preprocess, 306.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S8_left.jpg: 640x512 1 knife, 1 cell phone, 1 scissors, 283.6ms\n",
      "Speed: 1.4ms preprocess, 283.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S8_right.jpg: 640x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 1 scissors, 395.3ms\n",
      "Speed: 1.8ms preprocess, 395.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S9_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 340.0ms\n",
      "Speed: 1.9ms preprocess, 340.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S9_left.jpg: 608x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 345.8ms\n",
      "Speed: 1.8ms preprocess, 345.8ms inference, 0.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S9_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 dining table, 1 remote, 1 cell phone, 1 book, 1 scissors, 303.2ms\n",
      "Speed: 1.5ms preprocess, 303.2ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S10_front.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 scissors, 1 toothbrush, 356.8ms\n",
      "Speed: 1.5ms preprocess, 356.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S10_left.jpg: 640x480 1 cup, 1 fork, 1 knife, 1 cell phone, 1 scissors, 1 toothbrush, 273.5ms\n",
      "Speed: 1.6ms preprocess, 273.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Scenes/S10_right.jpg: 544x640 1 cup, 1 fork, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 1 toothbrush, 320.5ms\n",
      "Speed: 1.7ms preprocess, 320.5ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    }
   ],
   "source": [
    "# Clear the file before starting\n",
    "with open('../testing.txt', 'w') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "# Process each scene and perform detection\n",
    "for i in range(len(allowed_classes)):\n",
    "    for side in [\"front\", \"left\", \"right\"]:\n",
    "        scene_name = f\"S{i + 1}_{side}\"\n",
    "        image_path = f\"../Scenes/{scene_name}.jpg\"\n",
    "\n",
    "        # Perform object detection using the YOLO model\n",
    "        results = model(image_path)\n",
    "\n",
    "        # Calculate and log detection results to testing.txt\n",
    "        calculate_results(results, scene_name)\n",
    "\n",
    "        image = draw_bounding_box(results, image_path)\n",
    "\n",
    "        # Save the image with bounding boxes\n",
    "        output_path = f\"../Detected Objects/{scene_name}_bb.jpg\"\n",
    "        cv2.imwrite(output_path, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Object Detection and Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Results:\n",
      "Precision: 99.30%\n",
      "Recall: 86.06%\n",
      "F1-Score: 92.21%\n",
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoramic Image Stitching and Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class ImageStitching:\n",
    "    def __init__(self, q_image, t_image):\n",
    "        \"\"\"\n",
    "        Initializes the ImageStitching object and sets the smoothing window size.\n",
    "\n",
    "        Args:\n",
    "            q_image (numpy.ndarray): The first input image.\n",
    "            t_image (numpy.ndarray): The second input image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        q_photo_width = q_image.shape[1]    # Width of query photo\n",
    "        t_photo_width = t_image.shape[1]    # Width of training photo\n",
    "        min_width = min(q_photo_width, t_photo_width)\n",
    "        smoothing_window_factor = 0.10          # Smoothing window percentage [0.00, 1.00]\n",
    "        self.smoothing_window_size = max(\n",
    "            100, \n",
    "            min(smoothing_window_factor * min_width, 1000)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_grayscale(image):\n",
    "        \"\"\"\n",
    "        Converts an image to grayscale.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The original image and its grayscale version.\n",
    "        \"\"\"\n",
    "        photo_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        return image, photo_gray\n",
    "\n",
    "    @staticmethod\n",
    "    def sift_detector(image):\n",
    "        \"\"\"\n",
    "        Applies the SIFT algorithm to extract keypoints and features.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Keypoints and features extracted from the image.\n",
    "        \"\"\"\n",
    "        descriptor = cv2.SIFT_create()\n",
    "        keypoints, features = descriptor.detectAndCompute(image, None)\n",
    "        return keypoints, features\n",
    "\n",
    "    def create_and_match_keypoints(self, features_train_image, features_query_image):\n",
    "        \"\"\"\n",
    "        Matches keypoints from SIFT features using brute-force matching with L2 norm.\n",
    "\n",
    "        Args:\n",
    "            features_train_image (numpy.ndarray): SIFT features of the train image.\n",
    "            features_query_image (numpy.ndarray): SIFT features of the query image.\n",
    "\n",
    "        Returns:\n",
    "            list: Sorted list of raw matches based on distance.\n",
    "        \"\"\"\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "        best_matches = bf.match(features_train_image, features_query_image)\n",
    "        raw_matches = sorted(best_matches, key=lambda x: x.distance)\n",
    "        return raw_matches\n",
    "\n",
    "    def compute_homography(self, t_image_keypoints, q_image_keypoints, matches, reprojThresh):\n",
    "        \"\"\"\n",
    "        Computes the Homography matrix using matched keypoints and RANSAC.\n",
    "\n",
    "        Args:\n",
    "            t_image_keypoints (list): Keypoints from the train image.\n",
    "            q_image_keypoints (list): Keypoints from the query image.\n",
    "            matches (list): Matched keypoints between the two images.\n",
    "            reprojThresh (float): Threshold for RANSAC reprojection error.\n",
    "\n",
    "        Returns:\n",
    "            tuple or None: (matches, Homography matrix, status) if successful, else None.\n",
    "        \"\"\"\n",
    "        t_image_keypoints = np.float32([keypoint.pt for keypoint in t_image_keypoints])\n",
    "        q_image_keypoints = np.float32([keypoint.pt for keypoint in q_image_keypoints])\n",
    "\n",
    "        if len(matches) >= 4:\n",
    "            t_points = np.float32([t_image_keypoints[m.queryIdx] for m in matches])\n",
    "            q_points = np.float32([q_image_keypoints[m.trainIdx] for m in matches])\n",
    "\n",
    "            H, status = cv2.findHomography(t_points, q_points, cv2.RANSAC, reprojThresh)\n",
    "            return matches, H, status\n",
    "        else:\n",
    "            print(\"Minimum match count not satisfied. Cannot compute homography.\")\n",
    "            return None\n",
    "\n",
    "    def create_mask(self, query_image, train_image, version):\n",
    "        \"\"\"\n",
    "        Creates a mask for blending images using a smoothing window.\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy.ndarray): Query image.\n",
    "            train_image (numpy.ndarray): Train image.\n",
    "            version (str): Either 'left_image' or 'right_image' for the mask.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A 3-channel mask for blending.\n",
    "        \"\"\"\n",
    "        q_image_height = query_image.shape[0]\n",
    "        q_photo_width = query_image.shape[1]\n",
    "        t_photo_width = train_image.shape[1]\n",
    "\n",
    "        height_panorama = q_image_height\n",
    "        width_panorama = q_photo_width + t_photo_width\n",
    "\n",
    "        offset = int(self.smoothing_window_size / 2)\n",
    "        barrier = query_image.shape[1] - offset\n",
    "        mask = np.zeros((height_panorama, width_panorama))\n",
    "\n",
    "        if version == \"left_image\":\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(1, 0, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, : barrier - offset] = 1\n",
    "        else:\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(0, 1, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, barrier + offset :] = 1\n",
    "        return cv2.merge([mask, mask, mask])\n",
    "\n",
    "    def blending_smoothing(self, query_image, train_image, homography_matrix):\n",
    "        \"\"\"\n",
    "        Blends query and train images into a panorama using a homography matrix.\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy.ndarray): Query image.\n",
    "            train_image (numpy.ndarray): Train image.\n",
    "            homography_matrix (numpy.ndarray): Homography matrix to map images.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The resulting panoramic image.\n",
    "        \"\"\"\n",
    "        left_image_heigh = query_image.shape[0]\n",
    "        left_image_width = query_image.shape[1]\n",
    "        right_image_width = train_image.shape[1]\n",
    "\n",
    "        height_panorama = left_image_heigh\n",
    "        width_panorama = left_image_width + right_image_width\n",
    "\n",
    "        # Initialize the panorama and blend images\n",
    "        left_panorama = np.zeros((height_panorama, width_panorama, 3))\n",
    "        left_mask = self.create_mask(query_image, train_image, version=\"left_image\")\n",
    "        left_panorama[0 : query_image.shape[0], 0 : query_image.shape[1], :] = query_image\n",
    "        left_panorama *= left_mask\n",
    "\n",
    "        right_mask = self.create_mask(query_image, train_image, version=\"right_image\")\n",
    "        right_panorama = cv2.warpPerspective(\n",
    "            train_image, homography_matrix, (width_panorama, height_panorama)\n",
    "        ) * right_mask\n",
    "\n",
    "        # Combine the two panoramas\n",
    "        result = left_panorama + right_panorama\n",
    "\n",
    "        # Crop out extra black space\n",
    "        rows, cols = np.where(result[:, :, 0] != 0)\n",
    "        min_row, max_row = min(rows), max(rows) + 1\n",
    "        min_col, max_col = min(cols), max(cols) + 1\n",
    "\n",
    "        final_result = result[min_row:max_row, min_col:max_col, :]\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def forward_pass(q_image, t_image):\n",
    "    \"\"\"\n",
    "    Runs a forward pass using the ImageStitching class to stitch two images\n",
    "    into a seamless panorama. It includes grayscale conversion, feature detection,\n",
    "    keypoint matching, homography computation, and blending.\n",
    "\n",
    "    Args:\n",
    "        q_image (numpy.ndarray): The query image (left image for stitching).\n",
    "        t_image (numpy.ndarray): The train image (right image for stitching).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - result_rgb (numpy.ndarray): The final panoramic image in RGB format.\n",
    "            - mapped_feature_image_rgb (numpy.ndarray): A visualization of matched keypoints.\n",
    "        \n",
    "        If stitching fails, returns:\n",
    "            - str: Error message indicating stitching failure.\n",
    "    \"\"\"\n",
    "    # Initialize the ImageStitching object\n",
    "    image_stitching = ImageStitching(q_image, t_image)\n",
    "\n",
    "    # Convert both images to grayscale\n",
    "    _, q_image_gray = image_stitching.convert_grayscale(q_image)  # Left image\n",
    "    _, t_image_gray = image_stitching.convert_grayscale(t_image)  # Right image\n",
    "\n",
    "    # Detect keypoints and compute features using SIFT\n",
    "    t_image_keypoints, features_train_image = image_stitching.sift_detector(t_image_gray)\n",
    "    q_image_keypoints, features_query_image = image_stitching.sift_detector(q_image_gray)\n",
    "\n",
    "    # Match keypoints between the two images\n",
    "    matches = image_stitching.create_and_match_keypoints(features_train_image, features_query_image)\n",
    "\n",
    "    # Visualize the keypoint matches\n",
    "    mapped_feature_image = cv2.drawMatches(\n",
    "        t_image,\n",
    "        t_image_keypoints,\n",
    "        q_image,\n",
    "        q_image_keypoints,\n",
    "        matches[:100],  # Show the top 100 matches\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "\n",
    "    # Compute the homography matrix to map images to a common plane\n",
    "    M = image_stitching.compute_homography(t_image_keypoints, q_image_keypoints, matches, reprojThresh=4)\n",
    "    if M is None:\n",
    "        return \"Error cannot stitch images\"\n",
    "\n",
    "    (matches, homography_matrix, _) = M\n",
    "\n",
    "    # Blend the images to create the panorama\n",
    "    result = image_stitching.blending_smoothing(q_image, t_image, homography_matrix)\n",
    "\n",
    "    # Convert the mapped features and result to RGB for output\n",
    "    mapped_float_32 = np.float32(mapped_feature_image)\n",
    "    result_float32 = np.float32(result)\n",
    "    result_rgb = cv2.cvtColor(result_float32, cv2.COLOR_BGR2RGB)\n",
    "    mapped_feature_image_rgb = cv2.cvtColor(mapped_float_32, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return result_rgb, mapped_feature_image_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def stitch_images(image_list, no_of_images):\n",
    "    \"\"\"\n",
    "    Stitches two images from a provided list using the forward_pass function\n",
    "    to create a panoramic image.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): List of input images (numpy arrays).\n",
    "        no_of_images (int): Number of images in the list. The function uses the \n",
    "                            last two images in the list for stitching.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - result_rgb (numpy.ndarray): The final panoramic image in RGB format.\n",
    "            - mapped_image_rgb (numpy.ndarray): Visualization of matched keypoints.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `no_of_images` is less than 2 or if the image list does not contain enough images.\n",
    "    \"\"\"\n",
    "    # Ensure sufficient images are provided\n",
    "    if no_of_images < 2:\n",
    "        raise ValueError(\"At least two images are required for stitching.\")\n",
    "    if len(image_list) < no_of_images:\n",
    "        raise ValueError(\"The number of images in the list is less than `no_of_images`.\")\n",
    "\n",
    "    # Perform stitching using the last two images in the list\n",
    "    result, mapped_image = forward_pass(\n",
    "        q_image=image_list[no_of_images - 2],\n",
    "        t_image=image_list[no_of_images - 1],\n",
    "    )\n",
    "\n",
    "    # Convert mapped image to uint8 and RGB format for visualization\n",
    "    mapped_image_int8 = np.uint8(mapped_image)\n",
    "    mapped_image_rgb = cv2.cvtColor(mapped_image_int8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert result image to uint8 and RGB format for output\n",
    "    result_int8 = np.uint8(result)\n",
    "    result_rgb = cv2.cvtColor(result_int8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return result_rgb, mapped_image_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def panorama_main(image_list):\n",
    "    \"\"\"\n",
    "    Main function to create a panoramic image by stitching multiple images together.\n",
    "    Saves the final panorama and a visualization of keypoint matches in their respective directories.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): List of input images (numpy arrays) to be stitched sequentially.\n",
    "\n",
    "    Outputs:\n",
    "        - The final panoramic image is saved in the `../Panorama/` folder.\n",
    "        - The keypoint mapping visualization is saved in the `../Keypoints/` folder.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the `image_list` is empty or contains fewer than two images.\n",
    "    \"\"\"\n",
    "    # Ensure the image list is valid\n",
    "    if len(image_list) < 2:\n",
    "        raise ValueError(\"At least two images are required to create a panorama.\")\n",
    "\n",
    "    # Initialize the result with the first image\n",
    "    result = image_list[0]\n",
    "\n",
    "    for i in range(1, len(image_list)):\n",
    "        # Prepare a temporary list with the current result and the next image\n",
    "        temp_list = [result, image_list[i]]\n",
    "\n",
    "        # Stitch the images together\n",
    "        result, mapped_image = stitch_images(temp_list, len(temp_list))\n",
    "\n",
    "    # Create directories to save the outputs\n",
    "    os.makedirs(\"../Panorama\", exist_ok=True)\n",
    "    os.makedirs(\"../Keypoints\", exist_ok=True)\n",
    "\n",
    "    # Save the panoramic result\n",
    "    cv2.imwrite(\"../Panorama/Panorama.jpg\", result)\n",
    "    cv2.imwrite(\"../Keypoints/mapped_image.jpg\", mapped_image)\n",
    "\n",
    "    print(f\"Panoramic image saved!\")\n",
    "    print(f\"Keypoint mapping saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoramic image saved!\n",
      "Keypoint mapping saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Load images from the specified directory\n",
    "image_dir = \"../Panorama\"\n",
    "image_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    scene_name = f\"S{i + 1}\"\n",
    "    image_path = f\"{image_dir}/{scene_name}.jpg\"\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is not None:\n",
    "        image_list.append(img)\n",
    "    else:\n",
    "        print(f\"Image {scene_name}.jpg could not be loaded. Skipping.\")\n",
    "\n",
    "# Create the panorama image\n",
    "if len(image_list) > 1:\n",
    "    panorama_main(image_list)\n",
    "else:\n",
    "    print(\"Not enough images to create a panorama. Ensure at least two valid images are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/saifal-dinali/Desktop/Projects/Python Projects/OpenCV Projects/Image-Recognition-Project/Code/../Panorama/Panorama.jpg: 352x640 1 cup, 2 forks, 1 knife, 1 spoon, 1 remote, 1 cell phone, 1 book, 1 scissors, 1 toothbrush, 740.3ms\n",
      "Speed: 4.4ms preprocess, 740.3ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Panoramic image with bounding boxes saved at: ../Panorama/Panorama_bb.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Path to the generated panoramic image\n",
    "image_path = \"../Panorama/Panorama.jpg\"\n",
    "\n",
    "# Perform object detection using the model\n",
    "results = model(image_path)\n",
    "\n",
    "# Draw bounding boxes around detected objects\n",
    "image_with_bboxes = draw_bounding_box(results, image_path)\n",
    "\n",
    "# Save the image with bounding boxes to a new file\n",
    "output_path = \"../Panorama/Panorama_bb.jpg\"\n",
    "cv2.imwrite(output_path, image_with_bboxes)\n",
    "\n",
    "print(f\"Panoramic image with bounding boxes saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
