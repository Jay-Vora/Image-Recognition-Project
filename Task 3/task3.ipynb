{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class ImageStitching:\n",
    "    \"\"\"containts the utilities required to stitch images\"\"\"\n",
    "\n",
    "    def __init__(self, query_photo, train_photo):\n",
    "        super().__init__()\n",
    "        width_query_photo = query_photo.shape[1]\n",
    "        width_train_photo = train_photo.shape[1]\n",
    "        lowest_width = min(width_query_photo, width_train_photo)\n",
    "        smoothing_window_percent = 0.10 # consider increasing or decreasing[0.00, 1.00] \n",
    "        self.smoothing_window_size = max(100, min(smoothing_window_percent * lowest_width, 1000))\n",
    "\n",
    "    def give_gray(self, image):\n",
    "        \"\"\"receives an image array and returns grayscaled image\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): array of images\n",
    "\n",
    "        Returns:\n",
    "            image (numpy array): same as image input\n",
    "            photo_gray (numpy array): grayscaled images\n",
    "        \"\"\"\n",
    "        photo_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        return image, photo_gray\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_detector(image):\n",
    "        \"\"\"Applies SIFT algorithm to the given image\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): input image\n",
    "\n",
    "        Returns:\n",
    "            keypoints, features\n",
    "        \"\"\"\n",
    "        descriptor = cv2.SIFT_create()\n",
    "        keypoints, features = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        return keypoints, features\n",
    "\n",
    "    def create_and_match_keypoints(self, features_train_image, features_query_image):\n",
    "        \"\"\"Creates and Matches keypoints from the SIFT features using Brute Force matching\n",
    "        by checking the L2 norm of the feature vector\n",
    "\n",
    "        Args:\n",
    "            features_train_image: SIFT features of train image\n",
    "            features_query_image: SIFT features of query image\n",
    "\n",
    "        Returns:\n",
    "            matches (List): matches in features of train and query image\n",
    "        \"\"\"\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "        best_matches = bf.match(features_train_image, features_query_image)\n",
    "        raw_matches = sorted(best_matches, key=lambda x: x.distance)\n",
    "\n",
    "        return raw_matches\n",
    "\n",
    "    def compute_homography(\n",
    "        self, keypoints_train_image, keypoints_query_image, matches, reprojThresh\n",
    "    ):\n",
    "        \"\"\"Computes the Homography to map images to a single plane,\n",
    "        uses RANSAC algorithm to find the best matches iteratively.\n",
    "\n",
    "        Args:\n",
    "            keypoints_train_image: keypoints found using SIFT in train image\n",
    "            keypoints_query_image: keypoints found using SIFT in query image\n",
    "            matches: matches found using Brute Force\n",
    "            reprojThresh: threshold for error\n",
    "\n",
    "        Returns:\n",
    "            M (Tuple): (matches, Homography matrix, status)\n",
    "        \"\"\"\n",
    "        keypoints_train_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_train_image]\n",
    "        )\n",
    "        keypoints_query_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_query_image]\n",
    "        )\n",
    "\n",
    "        if len(matches) >= 4:\n",
    "            points_train = np.float32(\n",
    "                [keypoints_train_image[m.queryIdx] for m in matches]\n",
    "            )\n",
    "            points_query = np.float32(\n",
    "                [keypoints_query_image[m.trainIdx] for m in matches]\n",
    "            )\n",
    "\n",
    "            H, status = cv2.findHomography(\n",
    "                points_train, points_query, cv2.RANSAC, reprojThresh\n",
    "            )\n",
    "\n",
    "            return (matches, H, status)\n",
    "\n",
    "        else:\n",
    "            print(f\"Minimum match count not satisfied cannot get homopgrahy\")\n",
    "            return None\n",
    "\n",
    "    def create_mask(self, query_image, train_image, version):\n",
    "        \"\"\"Creates the mask using query and train images for blending the images,\n",
    "        using a gaussian smoothing window/kernel\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            version (str) == 'left_image' or 'right_image'\n",
    "\n",
    "        Returns:\n",
    "            masks\n",
    "        \"\"\"\n",
    "        height_query_photo = query_image.shape[0]\n",
    "        width_query_photo = query_image.shape[1]\n",
    "        width_train_photo = train_image.shape[1]\n",
    "        height_panorama = height_query_photo\n",
    "        width_panorama = width_query_photo + width_train_photo\n",
    "        offset = int(self.smoothing_window_size / 2)\n",
    "        barrier = query_image.shape[1] - int(self.smoothing_window_size / 2)\n",
    "        mask = np.zeros((height_panorama, width_panorama))\n",
    "        if version == \"left_image\":\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(1, 0, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, : barrier - offset] = 1\n",
    "        else:\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(0, 1, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, barrier + offset :] = 1\n",
    "        return cv2.merge([mask, mask, mask])\n",
    "\n",
    "    def blending_smoothing(self, query_image, train_image, homography_matrix):\n",
    "        \"\"\"blends both query and train image via the homography matrix,\n",
    "        and ensures proper blending and smoothing using masks created in create_masks()\n",
    "        to give a seamless panorama.\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            homography_matrix (numpy array): Homography to map images to a single plane\n",
    "\n",
    "        Returns:\n",
    "            panoramic image (numpy array)\n",
    "        \"\"\"\n",
    "        height_img1 = query_image.shape[0]\n",
    "        width_img1 = query_image.shape[1]\n",
    "        width_img2 = train_image.shape[1]\n",
    "        height_panorama = height_img1\n",
    "        width_panorama = width_img1 + width_img2\n",
    "\n",
    "        panorama1 = np.zeros((height_panorama, width_panorama, 3))\n",
    "        mask1 = self.create_mask(query_image, train_image, version=\"left_image\")\n",
    "        panorama1[0 : query_image.shape[0], 0 : query_image.shape[1], :] = query_image\n",
    "        panorama1 *= mask1\n",
    "        mask2 = self.create_mask(query_image, train_image, version=\"right_image\")\n",
    "        panorama2 = (\n",
    "            cv2.warpPerspective(\n",
    "                train_image, homography_matrix, (width_panorama, height_panorama)\n",
    "            )\n",
    "            * mask2\n",
    "        )\n",
    "        result = panorama1 + panorama2\n",
    "\n",
    "        # remove extra blackspace\n",
    "        rows, cols = np.where(result[:, :, 0] != 0)\n",
    "        min_row, max_row = min(rows), max(rows) + 1\n",
    "        min_col, max_col = min(cols), max(cols) + 1\n",
    "\n",
    "        final_result = result[min_row:max_row, min_col:max_col, :]\n",
    "\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def forward(query_photo, train_photo):\n",
    "    \"\"\"Runs a forward pass using the ImageStitching() class in utils.py.\n",
    "    Takes in a query image and train image and runs entire pipeline to return\n",
    "    a panoramic image.\n",
    "\n",
    "    Args:\n",
    "        query_photo (numpy array): query image\n",
    "        train_photo (nnumpy array): train image\n",
    "\n",
    "    Returns:\n",
    "        result image (numpy array): RGB result image\n",
    "    \"\"\"\n",
    "    image_stitching = ImageStitching(query_photo, train_photo)\n",
    "    _, query_photo_gray = image_stitching.give_gray(query_photo)  # left image\n",
    "    _, train_photo_gray = image_stitching.give_gray(train_photo)  # right image\n",
    "\n",
    "    keypoints_train_image, features_train_image = image_stitching._sift_detector(\n",
    "        train_photo_gray\n",
    "    )\n",
    "    keypoints_query_image, features_query_image = image_stitching._sift_detector(\n",
    "        query_photo_gray\n",
    "    )\n",
    "\n",
    "    matches = image_stitching.create_and_match_keypoints(\n",
    "        features_train_image, features_query_image\n",
    "    )\n",
    "\n",
    "    mapped_feature_image = cv2.drawMatches(\n",
    "                        train_photo,\n",
    "                        keypoints_train_image,\n",
    "                        query_photo,\n",
    "                        keypoints_query_image,\n",
    "                        matches[:100],\n",
    "                        None,\n",
    "                        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    M = image_stitching.compute_homography(\n",
    "        keypoints_train_image, keypoints_query_image, matches, reprojThresh=4\n",
    "    )\n",
    "\n",
    "    if M is None:\n",
    "        return \"Error cannot stitch images\"\n",
    "\n",
    "    (matches, homography_matrix, status) = M\n",
    "\n",
    "    result = image_stitching.blending_smoothing(\n",
    "        query_photo, train_photo, homography_matrix\n",
    "    )\n",
    "    # mapped_image = cv2.drawMatches(train_photo, keypoints_train_image, query_photo, keypoints_query_image, matches[:100], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    mapped_float_32 = np.float32(mapped_feature_image)\n",
    "    result_float32 = np.float32(result)\n",
    "    result_rgb = cv2.cvtColor(result_float32, cv2.COLOR_BGR2RGB)\n",
    "    mapped_feature_image_rgb = cv2.cvtColor(mapped_float_32, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return result_rgb, mapped_feature_image_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def stich_images(image_list, no_of_images):\n",
    "    result, mapped_image = forward(\n",
    "        query_photo=image_list[no_of_images - 2],\n",
    "        train_photo=image_list[no_of_images - 1],\n",
    "    )\n",
    "\n",
    "    mapped_image_int8 = np.uint8(mapped_image)\n",
    "    mapped_image_rgb = cv2.cvtColor(mapped_image_int8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result_int8 = np.uint8(result)\n",
    "    result_rgb = cv2.cvtColor(result_int8, cv2.COLOR_BGR2RGB)\n",
    "    return result_rgb, mapped_image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def read(image_dir_list):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(image_dir_list)):\n",
    "        img = cv2.imread(os.path.join(image_dir_list,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def main(image_dir):\n",
    "    \"\"\"Main function of the Repository.\n",
    "    Automatically uses the images in the specified directory \n",
    "    to create and export a panoramic image in the /outputs/ folder.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Directory containing input images\n",
    "    \"\"\"\n",
    "    # Read images from the specified directory\n",
    "    images_list = read(image_dir)\n",
    "\n",
    "    result = images_list[0]\n",
    "    temp_list = []\n",
    "    for i in range(1, len(images_list)):\n",
    "        temp_list = [result, images_list[i]]\n",
    "        # Process images to create a panorama\n",
    "        result, mapped_image = stich_images(temp_list, len(temp_list))\n",
    "        temp_list = []\n",
    "    \n",
    "    # Save the results to the outputs folder\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    cv2.imwrite(\"outputs/panorama_image.jpg\", result)\n",
    "    cv2.imwrite(\"outputs/mapped_image.jpg\", mapped_image)\n",
    "\n",
    "    print(f\"Panoramic image saved at: outputs/panorama_image.jpg\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoramic image saved at: outputs/panorama_image.jpg\n"
     ]
    }
   ],
   "source": [
    "# Automatically set the image directory to 'inputs/back'\n",
    "image_dir = \"inputs\"\n",
    "main(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img1 = cv2.imread('./inputs/1.jpeg')\n",
    "img2 = cv2.imread('./inputs/2.jpeg')\n",
    "img3 = cv2.imread('./inputs/3.jpeg')\n",
    "vis = np.concatenate((img1, img2, img3), axis=1)\n",
    "cv2.imwrite('out.jpeg', vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer Conv2D with weights [<KerasVariable shape=(7, 7, 3, 64), dtype=float32, path=conv1/kernel>, <KerasVariable shape=(64,), dtype=float32, path=conv1/bias>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpixellib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instance_segmentation\n\u001b[1;32m      4\u001b[0m segment_image \u001b[38;5;241m=\u001b[39m instance_segmentation()\n\u001b[0;32m----> 5\u001b[0m segment_image\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_rcnn_coco.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      6\u001b[0m segment_image\u001b[38;5;241m.\u001b[39msegmentImage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_image_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_new.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pixellib/instance/__init__.py:65\u001b[0m, in \u001b[0;36minstance_segmentation.load_model\u001b[0;34m(self, model_path, confidence)\u001b[0m\n\u001b[1;32m     61\u001b[0m     coco_config\u001b[38;5;241m.\u001b[39mDETECTION_MIN_CONFIDENCE \u001b[38;5;241m=\u001b[39m confidence\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m MaskRCNN(mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, config \u001b[38;5;241m=\u001b[39m coco_config)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_weights(model_path, by_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pixellib/instance/mask_rcnn.py:2110\u001b[0m, in \u001b[0;36mMaskRCNN.load_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2107\u001b[0m     layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m l: l\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude, layers)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[0;32m-> 2110\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group_by_name(f, layers)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2112\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group(f, layers)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/keras/saving/hdf5_format.py:766\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch)\u001b[0m\n\u001b[1;32m    763\u001b[0m weight_values \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(g[weight_name]) \u001b[38;5;28;01mfor\u001b[39;00m weight_name \u001b[38;5;129;01min\u001b[39;00m weight_names]\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m index\u001b[38;5;241m.\u001b[39mget(name, []):\n\u001b[0;32m--> 766\u001b[0m   symbolic_weights \u001b[38;5;241m=\u001b[39m _legacy_weights(layer)\n\u001b[1;32m    767\u001b[0m   weight_values \u001b[38;5;241m=\u001b[39m preprocess_weights_for_loading(\n\u001b[1;32m    768\u001b[0m       layer, weight_values, original_keras_version, original_backend)\n\u001b[1;32m    769\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(symbolic_weights):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/keras/saving/hdf5_format.py:895\u001b[0m, in \u001b[0;36m_legacy_weights\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    893\u001b[0m weights \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mtrainable_weights \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mnon_trainable_weights\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, variables_module\u001b[38;5;241m.\u001b[39mVariable) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights):\n\u001b[0;32m--> 895\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    896\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave or restore weights that is not an instance of `tf.Variable` is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    897\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot supported in h5, use `save_format=\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m` instead. Got a model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    898\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor layer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with weights \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, weights))\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer Conv2D with weights [<KerasVariable shape=(7, 7, 3, 64), dtype=float32, path=conv1/kernel>, <KerasVariable shape=(64,), dtype=float32, path=conv1/bias>]"
     ]
    }
   ],
   "source": [
    "import pixellib\n",
    "from pixellib.instance import instance_segmentation\n",
    "\n",
    "segment_image = instance_segmentation()\n",
    "segment_image.load_model(\"mask_rcnn_coco.h5\") \n",
    "segment_image.segmentImage(\"out.jpeg\", output_image_name = \"image_new.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
