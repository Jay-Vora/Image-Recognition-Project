{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class ImageStitching:\n",
    "    \"\"\"containts the utilities required to stitch images\"\"\"\n",
    "\n",
    "    def __init__(self, query_photo, train_photo):\n",
    "        super().__init__()\n",
    "        width_query_photo = query_photo.shape[1]\n",
    "        width_train_photo = train_photo.shape[1]\n",
    "        lowest_width = min(width_query_photo, width_train_photo)\n",
    "        smoothing_window_percent = 0.10 # consider increasing or decreasing[0.00, 1.00] \n",
    "        self.smoothing_window_size = max(100, min(smoothing_window_percent * lowest_width, 1000))\n",
    "\n",
    "    def give_gray(self, image):\n",
    "        \"\"\"receives an image array and returns grayscaled image\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): array of images\n",
    "\n",
    "        Returns:\n",
    "            image (numpy array): same as image input\n",
    "            photo_gray (numpy array): grayscaled images\n",
    "        \"\"\"\n",
    "        photo_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        return image, photo_gray\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_detector(image):\n",
    "        \"\"\"Applies SIFT algorithm to the given image\n",
    "\n",
    "        Args:\n",
    "            image (numpy array): input image\n",
    "\n",
    "        Returns:\n",
    "            keypoints, features\n",
    "        \"\"\"\n",
    "        descriptor = cv2.SIFT_create()\n",
    "        keypoints, features = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        return keypoints, features\n",
    "\n",
    "    def create_and_match_keypoints(self, features_train_image, features_query_image):\n",
    "        \"\"\"Creates and Matches keypoints from the SIFT features using Brute Force matching\n",
    "        by checking the L2 norm of the feature vector\n",
    "\n",
    "        Args:\n",
    "            features_train_image: SIFT features of train image\n",
    "            features_query_image: SIFT features of query image\n",
    "\n",
    "        Returns:\n",
    "            matches (List): matches in features of train and query image\n",
    "        \"\"\"\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "        best_matches = bf.match(features_train_image, features_query_image)\n",
    "        raw_matches = sorted(best_matches, key=lambda x: x.distance)\n",
    "\n",
    "        return raw_matches\n",
    "\n",
    "    def compute_homography(\n",
    "        self, keypoints_train_image, keypoints_query_image, matches, reprojThresh\n",
    "    ):\n",
    "        \"\"\"Computes the Homography to map images to a single plane,\n",
    "        uses RANSAC algorithm to find the best matches iteratively.\n",
    "\n",
    "        Args:\n",
    "            keypoints_train_image: keypoints found using SIFT in train image\n",
    "            keypoints_query_image: keypoints found using SIFT in query image\n",
    "            matches: matches found using Brute Force\n",
    "            reprojThresh: threshold for error\n",
    "\n",
    "        Returns:\n",
    "            M (Tuple): (matches, Homography matrix, status)\n",
    "        \"\"\"\n",
    "        keypoints_train_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_train_image]\n",
    "        )\n",
    "        keypoints_query_image = np.float32(\n",
    "            [keypoint.pt for keypoint in keypoints_query_image]\n",
    "        )\n",
    "\n",
    "        if len(matches) >= 4:\n",
    "            points_train = np.float32(\n",
    "                [keypoints_train_image[m.queryIdx] for m in matches]\n",
    "            )\n",
    "            points_query = np.float32(\n",
    "                [keypoints_query_image[m.trainIdx] for m in matches]\n",
    "            )\n",
    "\n",
    "            H, status = cv2.findHomography(\n",
    "                points_train, points_query, cv2.RANSAC, reprojThresh\n",
    "            )\n",
    "\n",
    "            return (matches, H, status)\n",
    "\n",
    "        else:\n",
    "            print(f\"Minimum match count not satisfied cannot get homopgrahy\")\n",
    "            return None\n",
    "\n",
    "    def create_mask(self, query_image, train_image, version):\n",
    "        \"\"\"Creates the mask using query and train images for blending the images,\n",
    "        using a gaussian smoothing window/kernel\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            version (str) == 'left_image' or 'right_image'\n",
    "\n",
    "        Returns:\n",
    "            masks\n",
    "        \"\"\"\n",
    "        height_query_photo = query_image.shape[0]\n",
    "        width_query_photo = query_image.shape[1]\n",
    "        width_train_photo = train_image.shape[1]\n",
    "        height_panorama = height_query_photo\n",
    "        width_panorama = width_query_photo + width_train_photo\n",
    "        offset = int(self.smoothing_window_size / 2)\n",
    "        barrier = query_image.shape[1] - int(self.smoothing_window_size / 2)\n",
    "        mask = np.zeros((height_panorama, width_panorama))\n",
    "        if version == \"left_image\":\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(1, 0, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, : barrier - offset] = 1\n",
    "        else:\n",
    "            mask[:, barrier - offset : barrier + offset] = np.tile(\n",
    "                np.linspace(0, 1, 2 * offset).T, (height_panorama, 1)\n",
    "            )\n",
    "            mask[:, barrier + offset :] = 1\n",
    "        return cv2.merge([mask, mask, mask])\n",
    "\n",
    "    def blending_smoothing(self, query_image, train_image, homography_matrix):\n",
    "        \"\"\"blends both query and train image via the homography matrix,\n",
    "        and ensures proper blending and smoothing using masks created in create_masks()\n",
    "        to give a seamless panorama.\n",
    "\n",
    "        Args:\n",
    "            query_image (numpy array)\n",
    "            train_image (numpy array)\n",
    "            homography_matrix (numpy array): Homography to map images to a single plane\n",
    "\n",
    "        Returns:\n",
    "            panoramic image (numpy array)\n",
    "        \"\"\"\n",
    "        height_img1 = query_image.shape[0]\n",
    "        width_img1 = query_image.shape[1]\n",
    "        width_img2 = train_image.shape[1]\n",
    "        height_panorama = height_img1\n",
    "        width_panorama = width_img1 + width_img2\n",
    "\n",
    "        panorama1 = np.zeros((height_panorama, width_panorama, 3))\n",
    "        mask1 = self.create_mask(query_image, train_image, version=\"left_image\")\n",
    "        panorama1[0 : query_image.shape[0], 0 : query_image.shape[1], :] = query_image\n",
    "        panorama1 *= mask1\n",
    "        mask2 = self.create_mask(query_image, train_image, version=\"right_image\")\n",
    "        panorama2 = (\n",
    "            cv2.warpPerspective(\n",
    "                train_image, homography_matrix, (width_panorama, height_panorama)\n",
    "            )\n",
    "            * mask2\n",
    "        )\n",
    "        result = panorama1 + panorama2\n",
    "\n",
    "        # remove extra blackspace\n",
    "        rows, cols = np.where(result[:, :, 0] != 0)\n",
    "        min_row, max_row = min(rows), max(rows) + 1\n",
    "        min_col, max_col = min(cols), max(cols) + 1\n",
    "\n",
    "        final_result = result[min_row:max_row, min_col:max_col, :]\n",
    "\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def forward(query_photo, train_photo):\n",
    "    \"\"\"Runs a forward pass using the ImageStitching() class in utils.py.\n",
    "    Takes in a query image and train image and runs entire pipeline to return\n",
    "    a panoramic image.\n",
    "\n",
    "    Args:\n",
    "        query_photo (numpy array): query image\n",
    "        train_photo (nnumpy array): train image\n",
    "\n",
    "    Returns:\n",
    "        result image (numpy array): RGB result image\n",
    "    \"\"\"\n",
    "    image_stitching = ImageStitching(query_photo, train_photo)\n",
    "    _, query_photo_gray = image_stitching.give_gray(query_photo)  # left image\n",
    "    _, train_photo_gray = image_stitching.give_gray(train_photo)  # right image\n",
    "\n",
    "    keypoints_train_image, features_train_image = image_stitching._sift_detector(\n",
    "        train_photo_gray\n",
    "    )\n",
    "    keypoints_query_image, features_query_image = image_stitching._sift_detector(\n",
    "        query_photo_gray\n",
    "    )\n",
    "\n",
    "    matches = image_stitching.create_and_match_keypoints(\n",
    "        features_train_image, features_query_image\n",
    "    )\n",
    "\n",
    "    mapped_feature_image = cv2.drawMatches(\n",
    "                        train_photo,\n",
    "                        keypoints_train_image,\n",
    "                        query_photo,\n",
    "                        keypoints_query_image,\n",
    "                        matches[:100],\n",
    "                        None,\n",
    "                        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    M = image_stitching.compute_homography(\n",
    "        keypoints_train_image, keypoints_query_image, matches, reprojThresh=4\n",
    "    )\n",
    "\n",
    "    if M is None:\n",
    "        return \"Error cannot stitch images\"\n",
    "\n",
    "    (matches, homography_matrix, status) = M\n",
    "\n",
    "    result = image_stitching.blending_smoothing(\n",
    "        query_photo, train_photo, homography_matrix\n",
    "    )\n",
    "    # mapped_image = cv2.drawMatches(train_photo, keypoints_train_image, query_photo, keypoints_query_image, matches[:100], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    mapped_float_32 = np.float32(mapped_feature_image)\n",
    "    result_float32 = np.float32(result)\n",
    "    result_rgb = cv2.cvtColor(result_float32, cv2.COLOR_BGR2RGB)\n",
    "    mapped_feature_image_rgb = cv2.cvtColor(mapped_float_32, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return result_rgb, mapped_feature_image_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def stich_images(image_list, no_of_images):\n",
    "    result, mapped_image = forward(\n",
    "        query_photo=image_list[no_of_images - 2],\n",
    "        train_photo=image_list[no_of_images - 1],\n",
    "    )\n",
    "\n",
    "    mapped_image_int8 = np.uint8(mapped_image)\n",
    "    mapped_image_rgb = cv2.cvtColor(mapped_image_int8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result_int8 = np.uint8(result)\n",
    "    result_rgb = cv2.cvtColor(result_int8, cv2.COLOR_BGR2RGB)\n",
    "    return result_rgb, mapped_image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def read(image_dir_list):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(image_dir_list)):\n",
    "        img = cv2.imread(os.path.join(image_dir_list,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def main(image_dir):\n",
    "    \"\"\"Main function of the Repository.\n",
    "    Automatically uses the images in the specified directory \n",
    "    to create and export a panoramic image in the /outputs/ folder.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Directory containing input images\n",
    "    \"\"\"\n",
    "    # Read images from the specified directory\n",
    "    images_list = read(image_dir)\n",
    "\n",
    "    result = images_list[0]\n",
    "    temp_list = []\n",
    "    for i in range(1, len(images_list)):\n",
    "        temp_list = [result, images_list[i]]\n",
    "        # Process images to create a panorama\n",
    "        result, mapped_image = stich_images(temp_list, len(temp_list))\n",
    "        temp_list = []\n",
    "    \n",
    "    # Save the results to the outputs folder\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    cv2.imwrite(\"outputs/panorama_image.jpg\", result)\n",
    "    cv2.imwrite(\"outputs/mapped_image.jpg\", mapped_image)\n",
    "\n",
    "    print(f\"Panoramic image saved at: outputs/panorama_image.jpg\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panoramic image saved at: outputs/panorama_image.jpg\n"
     ]
    }
   ],
   "source": [
    "# Automatically set the image directory to 'inputs/back'\n",
    "image_dir = \"inputs\"\n",
    "main(image_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
